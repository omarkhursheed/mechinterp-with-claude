{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a9fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set ALL the seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set all random seeds for complete reproducibility.\"\"\"\n",
    "    # Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy random\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # PyTorch CPU random\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # PyTorch CUDA random (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU\n",
    "        \n",
    "    # Make CuDNN deterministic for reproducibility (may impact performance)\n",
    "    if torch.backends.cudnn.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    print(\"Seeds set. Ready for deterministic exploration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f161998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367, 0.1288, 0.2345])\n",
      "tensor([0.3367, 0.1288, 0.2345])\n",
      "Identical? True\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    a = torch.randn(3)\n",
    "    print(a)\n",
    "    set_seed(42)\n",
    "    b = torch.randn(3)\n",
    "    print(b)\n",
    "    print(f\"Identical? {torch.equal(a, b)}\")  # Should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7b30cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=16, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads  # 16/2 = 8 dims per head\n",
    "        \n",
    "        # Create Q, K, V projections\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_Q(x)  # [batch, seq, d_model]\n",
    "        K = self.W_K(x)  # You implement\n",
    "        V = self.W_V(x) # You implement\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # We need [batch, n_heads, seq, d_head]\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2) # You reshape K the same way\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)  # You reshape V the same way\n",
    "        \n",
    "            # Compute attention scores\n",
    "        scores = Q @ K.transpose(2,3) / (self.d_head ** 0.5)        \n",
    "        # Apply attention to values\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # How do you turn scores into probabilities?\n",
    "        self.last_attn_weights = attn_weights  # Save for analysis!\n",
    "        attn_output = attn_weights @ V  # How do you apply weights to V?\n",
    "        attn_output = attn_output.transpose(1,2).reshape(batch_size, seq_len, d_model)\n",
    "\n",
    "        attn_output = self.W_O(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "983372f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model=16, n_heads=2, d_ff=64):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feedforward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Attention block with residual\n",
    "        attn_out, attn_weights = self.attn(x)\n",
    "        x = x + attn_out  # How do you combine x and attn_out (residual)?\n",
    "        x = self.norm1(x)  # Apply layer norm\n",
    "        \n",
    "        # Feedforward block with residual  \n",
    "        ff_out = self.ff(x)\n",
    "        x = x + ff_out  # Residual connection\n",
    "        x = self.norm2(x)  # Layer norm\n",
    "        \n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, d_model=16, n_heads=2, n_layers=2, vocab_size=10, seq_len=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, n_heads, d_ff=4*d_model) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Get embeddings\n",
    "        tok_emb = self.token_embed(x)\n",
    "        # Create positions on the same device as x\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.pos_embed(positions)\n",
    "        \n",
    "        # Combine\n",
    "        embeddings = tok_emb + pos_emb\n",
    "        \n",
    "        # Pass through layers, collecting attention weights\n",
    "        hidden = embeddings\n",
    "        attention_maps = []\n",
    "        for layer in self.layers:\n",
    "            hidden, attn_weights = layer(hidden)\n",
    "            attention_maps.append(attn_weights)\n",
    "            \n",
    "        logits = self.output(hidden)\n",
    "        return logits, attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "502fe1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: torch.Size([2, 8, 10])\n",
      "Attention maps: 2 layers, each torch.Size([2, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = TinyTransformer()\n",
    "test_input = torch.randint(0, 10, (2, 8))\n",
    "logits, attn_maps = model(test_input)\n",
    "print(f\"Logits: {logits.shape}\")\n",
    "print(f\"Attention maps: {len(attn_maps)} layers, each {attn_maps[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e3e803c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probe_sequences():\n",
    "    \"\"\"Create sequences to test what neurons respond to\"\"\"\n",
    "    probes = {}\n",
    "    \n",
    "    # Position probes - same token in different positions\n",
    "    probes['position'] = torch.tensor([[5,5,5,5,5,5,5,5]])  # All same token\n",
    "    \n",
    "    # Token identity probes - different tokens  \n",
    "    probes['token'] = torch.tensor([[0,1,2,3,4,5,6,7]])  # Different tokens\n",
    "    \n",
    "    # Specific position-token combinations\n",
    "    # Token 7 at position 3 only\n",
    "    probes['position_token'] = torch.tensor([[0,0,0,7,0,0,0,0]])\n",
    "    \n",
    "    return probes\n",
    "\n",
    "# ============================================================================\n",
    "# MECHANISTIC INTERPRETABILITY TOOLKIT\n",
    "# Day 1: Core observation and intervention tools\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ddeb7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ACTIVATION CACHE - Spy on the forward pass\n",
    "class ActivationCache:\n",
    "    \"\"\"\n",
    "    Non-invasive activation capture using PyTorch hooks.\n",
    "    This lets us observe what happens inside the model during forward passes.\n",
    "    \n",
    "    Key concepts:\n",
    "    - Hooks don't modify the computation, just observe\n",
    "    - We detach tensors to avoid memory issues\n",
    "    - Can capture any layer's input/output\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "    def create_hook(self, name):\n",
    "        \"\"\"Create a hook function that stores activations.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # TODO(human): Store the activation, but detach it from the computation graph\n",
    "            # Why detach? To save memory and prevent gradient tracking\n",
    "            # Hint: if output is a tuple (like attention returns), handle both cases\n",
    "            # Store in self.activations[name]\n",
    "            if isinstance(output, tuple):\n",
    "                self.activations[name] = output[0].clone().detach()\n",
    "            else:\n",
    "                self.activations[name] = output.clone().detach()\n",
    "\n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register hooks on specific layers we want to observe.\"\"\"\n",
    "        # Hook into attention layers\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            # Attention outputs\n",
    "            hook = layer.attn.register_forward_hook(self.create_hook(f'attn_{i}'))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "            # TODO(human): Also register a hook for the feedforward layer\n",
    "            # layer.ff is the feedforward network\n",
    "            # Name it f'ff_{i}'\n",
    "            hook = layer.ff.register_forward_hook(self.create_hook(f'ff_{i}'))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "        # Hook into embeddings\n",
    "        hook = model.token_embed.register_forward_hook(self.create_hook('embed'))\n",
    "        self.hooks.append(hook)\n",
    "        \n",
    "        # Hook into final output\n",
    "        hook = model.output.register_forward_hook(self.create_hook('logits'))\n",
    "        self.hooks.append(hook)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self.activations = {}\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks from the model.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"Easy access to activations.\"\"\"\n",
    "        return self.activations[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3oiw09oyw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOGIT LENS - Decode hidden states into vocabulary\n",
    "def logit_lens(model, hidden_states):\n",
    "    \"\"\"\n",
    "    The logit lens lets us interpret any hidden state as a distribution over tokens.\n",
    "    \n",
    "    Key insight: The residual stream at ANY point contains interpretable information!\n",
    "    We can decode it by applying the final output projection.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        hidden_states: Activations from any layer [batch, seq, d_model]\n",
    "    \n",
    "    Returns:\n",
    "        logits: Distribution over vocabulary [batch, seq, vocab_size]\n",
    "    \"\"\"\n",
    "    # Since TinyTransformer doesn't have a final LN before output,\n",
    "    # we directly project through the output layer\n",
    "    return model.output(hidden_states)\n",
    "\n",
    "def visualize_logit_lens(model, input_ids, cache, top_k=3):\n",
    "    \"\"\"\n",
    "    Visualize what tokens the model is 'thinking about' at each layer.\n",
    "    \n",
    "    Shows top-k predicted tokens at each position and layer.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Vocabulary for visualization (using indices as tokens since vocab_size=10)\n",
    "    vocab = [str(i) for i in range(10)]\n",
    "    \n",
    "    # Layers to analyze\n",
    "    layers_to_check = ['embed', 'attn_0', 'ff_0', 'attn_1', 'ff_1', 'logits']\n",
    "    \n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(len(layers_to_check), 1, figsize=(12, 2*len(layers_to_check)))\n",
    "    if len(layers_to_check) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for layer_idx, layer_name in enumerate(layers_to_check):\n",
    "        if layer_name in cache.activations:\n",
    "            # Get activations for this layer\n",
    "            activations = cache.activations[layer_name]\n",
    "            \n",
    "            if layer_name == 'logits':\n",
    "                # Already logits, no need to apply logit lens\n",
    "                logits = activations\n",
    "            else:\n",
    "                # Apply logit lens to get predictions\n",
    "                logits = logit_lens(model, activations)\n",
    "            \n",
    "            # Get top-k predictions for each position\n",
    "            probs = torch.softmax(logits[0], dim=-1)  # First batch item\n",
    "            top_probs, top_indices = torch.topk(probs, k=min(top_k, 10), dim=-1)\n",
    "            \n",
    "            # Create text representation\n",
    "            ax = axes[layer_idx]\n",
    "            ax.set_title(f\"Layer: {layer_name}\")\n",
    "            ax.set_xlim(0, seq_len)\n",
    "            ax.set_ylim(0, top_k)\n",
    "            ax.set_xlabel(\"Position\")\n",
    "            ax.set_ylabel(\"Top Predictions\")\n",
    "            \n",
    "            # Plot top predictions at each position\n",
    "            for pos in range(seq_len):\n",
    "                for k in range(min(top_k, 10)):\n",
    "                    token_id = top_indices[pos, k].item()\n",
    "                    prob = top_probs[pos, k].item()\n",
    "                    color_intensity = prob  # Use probability for color intensity\n",
    "                    \n",
    "                    # Display token with probability\n",
    "                    ax.text(pos + 0.5, top_k - k - 0.5, \n",
    "                           f\"{vocab[token_id]}\\n{prob:.2f}\",\n",
    "                           ha='center', va='center',\n",
    "                           fontsize=8,\n",
    "                           bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                                   facecolor=plt.cm.Blues(color_intensity),\n",
    "                                   alpha=0.7))\n",
    "            \n",
    "            ax.set_xticks(range(seq_len + 1))\n",
    "            ax.set_yticks([])\n",
    "            ax.grid(True, axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(\"Logit Lens: Model's Token Predictions Across Layers\", fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5gzwyy30he7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: [1, 2, 3, 4, 1, 2, 3, 4]\n",
      "Input shape: torch.Size([1, 8])\n",
      "\n",
      "üîç Logit Lens Analysis - What is the model thinking?\n",
      "--------------------------------------------------\n",
      "After embeddings, shape: torch.Size([1, 8, 10])\n",
      "  Top predicted token at last position: 1\n",
      "\n",
      "After attention layer 0:\n",
      "  Top predicted token at last position: 6\n",
      "\n",
      "After feedforward layer 0:\n",
      "  Top predicted token at last position: 6\n",
      "\n",
      "Final model output:\n",
      "  Top predicted token at last position: 8\n",
      "\n",
      "üí° Notice: Even a random model has preferences at each layer!\n",
      "This is evidence of pre-existing structure in random networks.\n"
     ]
    }
   ],
   "source": [
    "# Test the logit lens - see what the model is \"thinking\"\n",
    "set_seed(42)\n",
    "model = TinyTransformer()\n",
    "cache = ActivationCache()\n",
    "cache.register_hooks(model)\n",
    "\n",
    "# Create a simple test sequence\n",
    "test_input = torch.tensor([[1, 2, 3, 4, 1, 2, 3, 4]])  # Repeating pattern\n",
    "print(f\"Input sequence: {test_input[0].tolist()}\")\n",
    "print(f\"Input shape: {test_input.size()}\")\n",
    "# Run forward pass to collect activations\n",
    "logits, attn_maps = model(test_input)\n",
    "\n",
    "# Now use logit lens at different layers\n",
    "print(\"\\nüîç Logit Lens Analysis - What is the model thinking?\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check embeddings\n",
    "embed_logits = logit_lens(model, cache['embed'])\n",
    "print(f\"After embeddings, shape: {embed_logits.shape}\")\n",
    "top_tokens_embed = embed_logits[0, -1].argmax()  # Last position prediction\n",
    "print(f\"  Top predicted token at last position: {top_tokens_embed.item()}\")\n",
    "\n",
    "# Check after first attention layer\n",
    "attn0_logits = logit_lens(model, cache['attn_0'])\n",
    "top_tokens_attn0 = attn0_logits[0, -1].argmax()\n",
    "print(f\"\\nAfter attention layer 0:\")\n",
    "print(f\"  Top predicted token at last position: {top_tokens_attn0.item()}\")\n",
    "\n",
    "# Check after first feedforward\n",
    "ff0_logits = logit_lens(model, cache['ff_0'])\n",
    "top_tokens_ff0 = ff0_logits[0, -1].argmax()\n",
    "print(f\"\\nAfter feedforward layer 0:\")\n",
    "print(f\"  Top predicted token at last position: {top_tokens_ff0.item()}\")\n",
    "\n",
    "# Final output\n",
    "final_top = logits[0, -1].argmax()\n",
    "print(f\"\\nFinal model output:\")\n",
    "print(f\"  Top predicted token at last position: {final_top.item()}\")\n",
    "\n",
    "print(\"\\nüí° Notice: Even a random model has preferences at each layer!\")\n",
    "print(\"This is evidence of pre-existing structure in random networks.\")\n",
    "\n",
    "# Clean up\n",
    "cache.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feaexqhtwi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ATTENTION VISUALIZATION - See what the model is looking at\n",
    "def visualize_attention_patterns(attention_weights, input_ids=None, layer_idx=0, head_idx=None):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns to understand information flow.\n",
    "    \n",
    "    Common patterns to look for:\n",
    "    - Diagonal: attending to self\n",
    "    - Previous token: attending to the token before\n",
    "    - Induction: copying patterns from earlier in sequence\n",
    "    - Global: attending to specific positions (e.g., first token)\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: [batch, n_heads, seq_len, seq_len] or [batch, seq_len, seq_len]\n",
    "        input_ids: Optional token ids for labels\n",
    "        layer_idx: Which layer we're visualizing\n",
    "        head_idx: Specific head to visualize (None = average all heads)\n",
    "    \"\"\"\n",
    "    # Handle different shapes\n",
    "    if len(attention_weights.shape) == 4:\n",
    "        batch_size, n_heads, seq_len, _ = attention_weights.shape\n",
    "        if head_idx is not None:\n",
    "            attn = attention_weights[0, head_idx].detach().cpu().numpy()\n",
    "            title = f\"Layer {layer_idx}, Head {head_idx}\"\n",
    "        else:\n",
    "            # Average over heads\n",
    "            attn = attention_weights[0].mean(dim=0).detach().cpu().numpy()\n",
    "            title = f\"Layer {layer_idx}, Averaged Heads\"\n",
    "    else:\n",
    "        attn = attention_weights[0].detach().cpu().numpy()\n",
    "        title = f\"Layer {layer_idx}\"\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attn, cmap='Blues', cbar=True, square=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Position (attending to)\")\n",
    "    plt.ylabel(\"Position (attending from)\")\n",
    "    \n",
    "    # Add token labels if provided\n",
    "    if input_ids is not None:\n",
    "        tokens = input_ids[0].cpu().numpy()\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "        plt.yticks(range(len(tokens)), tokens)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def identify_attention_patterns(attention_weights, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Automatically identify common attention patterns.\n",
    "    \"\"\"\n",
    "    # Handle both 3D and 4D tensors\n",
    "    if len(attention_weights.shape) == 4:\n",
    "        # [batch, n_heads, seq, seq]\n",
    "        batch, n_heads, seq_len, _ = attention_weights.shape\n",
    "        attn = attention_weights[0]  # Take first batch item\n",
    "    else:\n",
    "        # [batch, seq, seq] - extract seq_len here for 3D case\n",
    "        batch, seq_len, _ = attention_weights.shape\n",
    "        attn = attention_weights[0].unsqueeze(0)  # Add head dimension\n",
    "        n_heads = 1\n",
    "\n",
    "    patterns = {\n",
    "        'diagonal': [],\n",
    "        'previous': [],\n",
    "        'first': [],\n",
    "        'uniform': []\n",
    "    }\n",
    "\n",
    "    for head_idx in range(attn.shape[0]):\n",
    "        head_attn = attn[head_idx]  # [seq, seq]\n",
    "\n",
    "        # Diagonal (self-attention)\n",
    "        diagonal_avg = head_attn.diagonal().mean().item()\n",
    "        patterns['diagonal'].append(diagonal_avg)\n",
    "\n",
    "        # Previous token (offset=-1 diagonal)\n",
    "        if seq_len > 1:\n",
    "            prev_diag = head_attn.diagonal(offset=-1)\n",
    "            previous_avg = prev_diag.mean().item() if len(prev_diag) > 0 else 0\n",
    "        else:\n",
    "            previous_avg = 0\n",
    "        patterns['previous'].append(previous_avg)\n",
    "\n",
    "        # First token attention (column 0)\n",
    "        first_avg = head_attn[:, 0].mean().item()\n",
    "        patterns['first'].append(first_avg)\n",
    "\n",
    "        # Uniformity (lower std = more uniform)\n",
    "        # Perfect uniform would be 1/seq_len everywhere, std would be 0\n",
    "        uniformity = 1 - head_attn.std().item()  # Simple std across all values\n",
    "        patterns['uniform'].append(uniformity)\n",
    "\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ux5e958d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Attention Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "REPEATING sequence: [1, 2, 3, 4, 1, 2, 3, 4]\n",
      "----------------------------------------\n",
      "\n",
      "Layer 0:\n",
      "  Head 0:\n",
      "    Self-attention: 0.138\n",
      "    Previous token: 0.105\n",
      "    First token:    0.141\n",
      "    Uniformity:     0.941\n",
      "    ‚Üí Dominant: first attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.078\n",
      "    Previous token: 0.117\n",
      "    First token:    0.087\n",
      "    Uniformity:     0.930\n",
      "    ‚Üí Dominant: previous attention\n",
      "\n",
      "Layer 1:\n",
      "  Head 0:\n",
      "    Self-attention: 0.119\n",
      "    Previous token: 0.143\n",
      "    First token:    0.129\n",
      "    Uniformity:     0.937\n",
      "    ‚Üí Dominant: previous attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.117\n",
      "    Previous token: 0.117\n",
      "    First token:    0.109\n",
      "    Uniformity:     0.968\n",
      "    ‚Üí Dominant: self attention\n",
      "\n",
      "ASCENDING sequence: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "----------------------------------------\n",
      "\n",
      "Layer 0:\n",
      "  Head 0:\n",
      "    Self-attention: 0.121\n",
      "    Previous token: 0.121\n",
      "    First token:    0.127\n",
      "    Uniformity:     0.913\n",
      "    ‚Üí Dominant: first attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.083\n",
      "    Previous token: 0.107\n",
      "    First token:    0.074\n",
      "    Uniformity:     0.906\n",
      "    ‚Üí Dominant: previous attention\n",
      "\n",
      "Layer 1:\n",
      "  Head 0:\n",
      "    Self-attention: 0.105\n",
      "    Previous token: 0.148\n",
      "    First token:    0.091\n",
      "    Uniformity:     0.937\n",
      "    ‚Üí Dominant: previous attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.118\n",
      "    Previous token: 0.135\n",
      "    First token:    0.127\n",
      "    Uniformity:     0.958\n",
      "    ‚Üí Dominant: previous attention\n",
      "\n",
      "CONSTANT sequence: [5, 5, 5, 5, 5, 5, 5, 5]\n",
      "----------------------------------------\n",
      "\n",
      "Layer 0:\n",
      "  Head 0:\n",
      "    Self-attention: 0.127\n",
      "    Previous token: 0.109\n",
      "    First token:    0.121\n",
      "    Uniformity:     0.967\n",
      "    ‚Üí Dominant: self attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.092\n",
      "    Previous token: 0.130\n",
      "    First token:    0.150\n",
      "    Uniformity:     0.921\n",
      "    ‚Üí Dominant: first attention\n",
      "\n",
      "Layer 1:\n",
      "  Head 0:\n",
      "    Self-attention: 0.127\n",
      "    Previous token: 0.141\n",
      "    First token:    0.123\n",
      "    Uniformity:     0.969\n",
      "    ‚Üí Dominant: previous attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.125\n",
      "    Previous token: 0.133\n",
      "    First token:    0.154\n",
      "    Uniformity:     0.970\n",
      "    ‚Üí Dominant: first attention\n",
      "\n",
      "üí° Even random models show structured attention patterns!\n"
     ]
    }
   ],
   "source": [
    "# Test attention pattern identification\n",
    "set_seed(42)\n",
    "model = TinyTransformer()\n",
    "\n",
    "# Create different test sequences to see different patterns\n",
    "test_sequences = {\n",
    "    'repeating': torch.tensor([[1, 2, 3, 4, 1, 2, 3, 4]]),\n",
    "    'ascending': torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7]]),\n",
    "    'constant': torch.tensor([[5, 5, 5, 5, 5, 5, 5, 5]])\n",
    "}\n",
    "\n",
    "print(\"üîç Attention Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for seq_name, seq in test_sequences.items():\n",
    "    print(f\"\\n{seq_name.upper()} sequence: {seq[0].tolist()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get attention patterns\n",
    "    logits, attn_maps = model(seq)\n",
    "    \n",
    "    # Analyze each layer\n",
    "    for layer_idx, attn in enumerate(attn_maps):\n",
    "        patterns = identify_attention_patterns(attn)\n",
    "        \n",
    "        print(f\"\\nLayer {layer_idx}:\")\n",
    "        for head_idx in range(len(patterns['diagonal'])):\n",
    "            print(f\"  Head {head_idx}:\")\n",
    "            print(f\"    Self-attention: {patterns['diagonal'][head_idx]:.3f}\")\n",
    "            print(f\"    Previous token: {patterns['previous'][head_idx]:.3f}\")\n",
    "            print(f\"    First token:    {patterns['first'][head_idx]:.3f}\")\n",
    "            print(f\"    Uniformity:     {patterns['uniform'][head_idx]:.3f}\")\n",
    "            \n",
    "            # Identify dominant pattern\n",
    "            scores = {\n",
    "                'self': patterns['diagonal'][head_idx],\n",
    "                'previous': patterns['previous'][head_idx],\n",
    "                'first': patterns['first'][head_idx]\n",
    "            }\n",
    "            dominant = max(scores, key=scores.get)\n",
    "            print(f\"    ‚Üí Dominant: {dominant} attention\")\n",
    "\n",
    "print(\"\\nüí° Even random models show structured attention patterns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "az4uvhz2dyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured activations:\n",
      "  embed: torch.Size([1, 8, 16])\n",
      "  attn_0: torch.Size([1, 8, 16])\n",
      "  ff_0: torch.Size([1, 8, 16])\n",
      "  attn_1: torch.Size([1, 8, 16])\n",
      "  ff_1: torch.Size([1, 8, 16])\n",
      "  logits: torch.Size([1, 8, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST YOUR IMPLEMENTATION - Morning checkpoint\n",
    "def test_hooks_and_cache():\n",
    "    \"\"\"Test that your hook implementation works correctly.\"\"\"\n",
    "    set_seed(42)\n",
    "    model = TinyTransformer()\n",
    "    cache = ActivationCache()\n",
    "    \n",
    "    # Register hooks\n",
    "    cache.register_hooks(model)\n",
    "    \n",
    "    # Run forward pass\n",
    "    test_input = torch.randint(0, 10, (1, 8))\n",
    "    logits, attn_maps = model(test_input)\n",
    "    \n",
    "    # Check that activations were captured\n",
    "    print(\"Captured activations:\")\n",
    "    for key in cache.activations:\n",
    "        if isinstance(cache.activations[key], tuple):\n",
    "            print(f\"  {key}: {cache.activations[key][0].shape}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {cache.activations[key].shape}\")\n",
    "    \n",
    "    # Clean up\n",
    "    cache.remove_hooks()\n",
    "    return cache\n",
    "\n",
    "# Run this after implementing the TODOs above\n",
    "cache = test_hooks_and_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2slpswuy0py",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: [7, 7, 0, 5, 5, 3, 0, 3]\n",
      "\n",
      "‚úÖ Successfully captured activations:\n",
      "  attn_0: torch.Size([1, 8, 16])\n",
      "  attn_1: torch.Size([1, 8, 16])\n",
      "  embed: torch.Size([1, 8, 16])\n",
      "  ff_0: torch.Size([1, 8, 16])\n",
      "  ff_1: torch.Size([1, 8, 16])\n",
      "  logits: torch.Size([1, 8, 10])\n",
      "\n",
      "üéâ Hooks working perfectly! You can now observe any layer's activations.\n"
     ]
    }
   ],
   "source": [
    "# Let's test your hook implementation!\n",
    "set_seed(42)\n",
    "model = TinyTransformer()\n",
    "cache = ActivationCache()\n",
    "\n",
    "# Register hooks\n",
    "cache.register_hooks(model)\n",
    "\n",
    "# Run forward pass\n",
    "test_input = torch.randint(0, 10, (1, 8))\n",
    "print(f\"Input sequence: {test_input[0].tolist()}\")\n",
    "\n",
    "logits, attn_maps = model(test_input)\n",
    "\n",
    "# Check what we captured\n",
    "print(\"\\n‚úÖ Successfully captured activations:\")\n",
    "for key in sorted(cache.activations.keys()):\n",
    "    act = cache.activations[key]\n",
    "    if isinstance(act, tuple):\n",
    "        shape = act[0].shape if hasattr(act[0], 'shape') else type(act[0])\n",
    "    else:\n",
    "        shape = act.shape if hasattr(act, 'shape') else type(act)\n",
    "    print(f\"  {key}: {shape}\")\n",
    "\n",
    "# Clean up\n",
    "cache.remove_hooks()\n",
    "\n",
    "print(\"\\nüéâ Hooks working perfectly! You can now observe any layer's activations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5q4y119fdl9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: INTERVENTION TOOLS - Understanding causality\n",
    "# ============================================================================\n",
    "\n",
    "# 4. ACTIVATION PATCHING - Test causal importance\n",
    "def activation_patching(model, clean_input, corrupted_input, layer_name, cache_clean, cache_corrupted, metric_fn):\n",
    "    \"\"\"\n",
    "    Activation patching reveals which components are causally important.\n",
    "    \n",
    "    Method:\n",
    "    1. Run model on corrupted input (e.g., with critical info removed)\n",
    "    2. Run model on clean input\n",
    "    3. Replace specific activation from corrupted run with clean activation\n",
    "    4. If performance recovers, that activation was crucial!\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        clean_input: Input with correct information\n",
    "        corrupted_input: Input with information removed/corrupted\n",
    "        layer_name: Which activation to patch (e.g., 'attn_0', 'ff_1')\n",
    "        cache_clean: ActivationCache from clean run\n",
    "        cache_corrupted: ActivationCache from corrupted run (for consistency, not used)\n",
    "        metric_fn: Function to measure performance (higher = better)\n",
    "    \n",
    "    Returns:\n",
    "        patched_metric: Performance after patching\n",
    "        baseline_metric: Performance without patching (corrupted)\n",
    "        clean_metric: Performance on clean input (upper bound)\n",
    "    \"\"\"\n",
    "    # Get clean activation for patching\n",
    "    clean_activation = cache_clean[layer_name]\n",
    "    \n",
    "    # Create a patching hook that will replace activations during forward pass\n",
    "    def patching_hook(module, input, output):\n",
    "        if isinstance(output, tuple):\n",
    "            # For attention layers that return (output, attention_weights)\n",
    "            output = (clean_activation, output[1])\n",
    "        else:\n",
    "            # For other layers that return just the activation\n",
    "            output = clean_activation\n",
    "        return output\n",
    "    \n",
    "    # Register the patching hook on the appropriate layer\n",
    "    target_module = None\n",
    "    if 'attn' in layer_name:\n",
    "        layer_idx = int(layer_name.split('_')[1])\n",
    "        target_module = model.layers[layer_idx].attn\n",
    "    elif 'ff' in layer_name:\n",
    "        layer_idx = int(layer_name.split('_')[1])\n",
    "        target_module = model.layers[layer_idx].ff\n",
    "    elif layer_name == 'embed':\n",
    "        target_module = model.token_embed\n",
    "    \n",
    "    # Register hook and run patched forward pass\n",
    "    hook_handle = target_module.register_forward_hook(patching_hook)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Run on corrupted input with patching\n",
    "        patched_output, _ = model(corrupted_input)\n",
    "        patched_metric = metric_fn(patched_output, corrupted_input)\n",
    "        \n",
    "        # Clean up hook\n",
    "        hook_handle.remove()\n",
    "        \n",
    "        # Get baseline metrics\n",
    "        corrupted_output, _ = model(corrupted_input)\n",
    "        baseline_metric = metric_fn(corrupted_output, corrupted_input)\n",
    "        \n",
    "        clean_output, _ = model(clean_input)\n",
    "        clean_metric = metric_fn(clean_output, clean_input)\n",
    "    \n",
    "    return patched_metric, baseline_metric, clean_metric\n",
    "\n",
    "# 5. ABLATION STUDIES - Remove components to test importance\n",
    "def ablate_neurons(model, layer_name, neuron_indices, replacement='zero', mean_cache=None):\n",
    "    \"\"\"\n",
    "    Ablate (remove) specific neurons to test their importance.\n",
    "    \n",
    "    Replacement strategies:\n",
    "    - 'zero': Set activations to 0\n",
    "    - 'mean': Replace with mean activation (requires mean_cache)\n",
    "    - 'random': Replace with random noise\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        layer_name: Which layer to ablate (e.g., 'ff_0')\n",
    "        neuron_indices: List of neuron indices to ablate\n",
    "        replacement: How to replace ablated neurons\n",
    "        mean_cache: Dict with mean activations for each layer (required for 'mean' strategy)\n",
    "    \"\"\"\n",
    "    def ablation_hook(module, input, output):\n",
    "        # Handle tuple case (attention layers)\n",
    "        if isinstance(output, tuple):\n",
    "            # Clone the first element (the actual activation)\n",
    "            modified = output[0].clone()\n",
    "            # Apply ablation to specific neurons\n",
    "            if replacement == 'zero':\n",
    "                modified[..., neuron_indices] = 0\n",
    "            elif replacement == 'mean':\n",
    "                if mean_cache and layer_name in mean_cache:\n",
    "                    # Replace with pre-computed mean values for those specific neurons\n",
    "                    mean_values = mean_cache[layer_name][neuron_indices]\n",
    "                    # Broadcast mean values to match modified shape [batch, seq, hidden]\n",
    "                    modified[..., neuron_indices] = mean_values\n",
    "                else:\n",
    "                    # Fallback: use mean across current batch and sequence\n",
    "                    mean_val = modified.mean(dim=(0, 1))  # [hidden]\n",
    "                    modified[..., neuron_indices] = mean_val[neuron_indices]\n",
    "            elif replacement == 'random':\n",
    "                modified[..., neuron_indices] = torch.randn_like(modified[..., neuron_indices])\n",
    "            # Return as tuple with unchanged attention weights\n",
    "            return (modified, output[1])\n",
    "        else:\n",
    "            # Clone for regular layers\n",
    "            modified = output.clone()\n",
    "            # Apply ablation to specific neurons\n",
    "            if replacement == 'zero':\n",
    "                modified[..., neuron_indices] = 0\n",
    "            elif replacement == 'mean':\n",
    "                if mean_cache and layer_name in mean_cache:\n",
    "                    # Replace with pre-computed mean values for those specific neurons\n",
    "                    mean_values = mean_cache[layer_name][neuron_indices]\n",
    "                    # Broadcast to match shape [batch, seq, hidden]\n",
    "                    modified[..., neuron_indices] = mean_values\n",
    "                else:\n",
    "                    # Fallback: use mean across current batch and sequence\n",
    "                    mean_val = modified.mean(dim=(0, 1))  # [hidden]\n",
    "                    modified[..., neuron_indices] = mean_val[neuron_indices]\n",
    "            elif replacement == 'random':\n",
    "                modified[..., neuron_indices] = torch.randn_like(modified[..., neuron_indices])\n",
    "            return modified\n",
    "    \n",
    "    # Find the target module\n",
    "    target_module = None\n",
    "    if 'attn' in layer_name:\n",
    "        layer_idx = int(layer_name.split('_')[1])\n",
    "        target_module = model.layers[layer_idx].attn\n",
    "    elif 'ff' in layer_name:\n",
    "        layer_idx = int(layer_name.split('_')[1])\n",
    "        target_module = model.layers[layer_idx].ff\n",
    "    elif layer_name == 'embed':\n",
    "        target_module = model.token_embed\n",
    "    \n",
    "    # Register and return the hook handle\n",
    "    if target_module:\n",
    "        return target_module.register_forward_hook(ablation_hook)\n",
    "    return None\n",
    "\n",
    "def compute_mean_cache(model, dataloader=None):\n",
    "    \"\"\"\n",
    "    Compute mean activations for each layer across a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        dataloader: DataLoader or list of input tensors (if None, uses random data)\n",
    "    \n",
    "    Returns:\n",
    "        mean_cache: Dict mapping layer names to mean activations with shape [hidden]\n",
    "    \"\"\"\n",
    "    if dataloader is None:\n",
    "        # Create some random data as fallback\n",
    "        dataloader = [torch.randint(0, 10, (4, 8)) for _ in range(10)]\n",
    "    \n",
    "    # Set up activation cache\n",
    "    cache = ActivationCache()\n",
    "    cache.register_hooks(model)\n",
    "    \n",
    "    # Accumulate activations\n",
    "    activation_sums = {}\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if isinstance(batch, (tuple, list)):\n",
    "                batch = batch[0]  # Handle (input, label) format\n",
    "            \n",
    "            # Forward pass\n",
    "            _ = model(batch)\n",
    "            \n",
    "            # Accumulate activations\n",
    "            for key, act in cache.activations.items():\n",
    "                if isinstance(act, tuple):\n",
    "                    act = act[0]\n",
    "                \n",
    "                if key not in activation_sums:\n",
    "                    # Initialize with zeros of correct shape [hidden]\n",
    "                    hidden_dim = act.shape[-1]\n",
    "                    activation_sums[key] = torch.zeros(hidden_dim, device=act.device)\n",
    "                \n",
    "                # Sum across batch and sequence dimensions to get [hidden]\n",
    "                activation_sums[key] += act.mean(dim=(0, 1))\n",
    "            \n",
    "            count += 1\n",
    "            cache.clear()\n",
    "    \n",
    "    # Compute means\n",
    "    mean_cache = {}\n",
    "    for key, sum_act in activation_sums.items():\n",
    "        mean_cache[key] = sum_act / count\n",
    "    \n",
    "    # Clean up\n",
    "    cache.remove_hooks()\n",
    "    \n",
    "    return mean_cache\n",
    "\n",
    "def measure_ablation_impact(model, input_ids, layer_name, neuron_idx, metric_fn, mean_cache=None):\n",
    "    \"\"\"\n",
    "    Measure how much a specific neuron matters.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        input_ids: Input tensor\n",
    "        layer_name: Which layer to ablate\n",
    "        neuron_idx: Which neuron to ablate\n",
    "        metric_fn: Function to measure performance\n",
    "        mean_cache: Optional pre-computed mean activations for mean ablation\n",
    "    \n",
    "    Returns:\n",
    "        impact: Change in metric when neuron is ablated\n",
    "    \"\"\"\n",
    "    # Baseline performance\n",
    "    with torch.no_grad():\n",
    "        baseline_output = model(input_ids)[0]\n",
    "        baseline_metric = metric_fn(baseline_output, input_ids)\n",
    "    \n",
    "    # Performance with neuron ablated\n",
    "    hook_handle = ablate_neurons(model, layer_name, [neuron_idx], replacement='zero', mean_cache=mean_cache)\n",
    "    with torch.no_grad():\n",
    "        modified_output = model(input_ids)[0]\n",
    "        modified_metric = metric_fn(modified_output, input_ids)\n",
    "    hook_handle.remove()\n",
    "    \n",
    "    return baseline_metric - modified_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "jetxhkf0mwd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MECHANISTIC INTERPRETABILITY ANALYSIS PIPELINE\n",
      "============================================================\n",
      "\n",
      "1. OBSERVING WITH HOOKS\n",
      "----------------------------------------\n",
      "Captured activations from forward pass:\n",
      "  attn_0: torch.Size([1, 8, 16])\n",
      "  attn_1: torch.Size([1, 8, 16])\n",
      "  embed: torch.Size([1, 8, 16])\n",
      "  ff_0: torch.Size([1, 8, 16])\n",
      "  ff_1: torch.Size([1, 8, 16])\n",
      "  logits: torch.Size([1, 8, 10])\n",
      "\n",
      "2. ATTENTION PATTERN ANALYSIS\n",
      "----------------------------------------\n",
      "Visualizing Layer 0 attention patterns...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Visualization would appear here)\n",
      "\n",
      "3. LOGIT LENS ANALYSIS\n",
      "----------------------------------------\n",
      "What is the model 'thinking' at each layer?\n",
      "\n",
      "4. INTERVENTION EXPERIMENTS\n",
      "----------------------------------------\n",
      "Testing causal importance via patching...\n",
      "\n",
      "5. ABLATION STUDIES\n",
      "----------------------------------------\n",
      "Testing neuron importance via ablation...\n",
      "\n",
      "============================================================\n",
      "Analysis complete! Ready for your research experiments.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE: How to use these tools together\n",
    "def example_analysis_pipeline():\n",
    "    \"\"\"\n",
    "    Example showing how to combine all the tools for analysis.\n",
    "    This is what you'll do in your research!\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MECHANISTIC INTERPRETABILITY ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Setup model and inputs\n",
    "    set_seed(42)\n",
    "    model = TinyTransformer()\n",
    "    \n",
    "    # Create interesting test sequences\n",
    "    # Induction: pattern that repeats\n",
    "    induction_seq = torch.tensor([[1, 2, 3, 4, 1, 2, 3, 4]])  \n",
    "    # Random sequence for comparison\n",
    "    random_seq = torch.randint(0, 10, (1, 8))\n",
    "    \n",
    "    print(\"\\n1. OBSERVING WITH HOOKS\")\n",
    "    print(\"-\" * 40)\n",
    "    # Set up activation cache\n",
    "    cache = ActivationCache()\n",
    "    cache.register_hooks(model)\n",
    "    \n",
    "    # Run forward pass\n",
    "    logits, attn_maps = model(induction_seq)\n",
    "    \n",
    "    print(\"Captured activations from forward pass:\")\n",
    "    for key in sorted(cache.activations.keys()):\n",
    "        act = cache.activations[key]\n",
    "        if isinstance(act, tuple):\n",
    "            print(f\"  {key}: {act[0].shape if hasattr(act[0], 'shape') else type(act[0])}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {act.shape if hasattr(act, 'shape') else type(act)}\")\n",
    "    \n",
    "    print(\"\\n2. ATTENTION PATTERN ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    # Visualize attention for first layer\n",
    "    if len(attn_maps) > 0:\n",
    "        print(\"Visualizing Layer 0 attention patterns...\")\n",
    "        visualize_attention_patterns(attn_maps[0], induction_seq, layer_idx=0)\n",
    "        print(\"  (Visualization would appear here)\")\n",
    "    \n",
    "    print(\"\\n3. LOGIT LENS ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"What is the model 'thinking' at each layer?\")\n",
    "    # This would apply logit lens to intermediate activations\n",
    "    # For each layer: logit_lens(model, cache[f'ff_{i}'])\n",
    "    \n",
    "    print(\"\\n4. INTERVENTION EXPERIMENTS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing causal importance via patching...\")\n",
    "    # Example: corrupt the sequence and patch to test importance\n",
    "    corrupted = torch.randint(0, 10, (1, 8))  # Random corruption\n",
    "    # Would run: activation_patching(model, induction_seq, corrupted, 'attn_0', ...)\n",
    "    \n",
    "    print(\"\\n5. ABLATION STUDIES\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing neuron importance via ablation...\")\n",
    "    # Would run: measure_ablation_impact(model, induction_seq, 'ff_0', neuron_idx=0, ...)\n",
    "    \n",
    "    # Clean up\n",
    "    cache.remove_hooks()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Analysis complete! Ready for your research experiments.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Uncomment to run the example (after implementing TODOs):\n",
    "example_analysis_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8y0wusmt9lq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Complete Mechanistic Interpretability Toolkit\n",
      "============================================================\n",
      "\n",
      "1. ACTIVATION PATCHING TEST\n",
      "----------------------------------------\n",
      "  Baseline (corrupted): -2.1558\n",
      "  After patching attn_0: -2.1562\n",
      "  Clean target:         -2.1711\n",
      "  Patching works!\n",
      "\n",
      "2. ABLATION STUDY TEST\n",
      "----------------------------------------\n",
      "  Computing mean activation cache...\n",
      "  Testing neuron importance in ff_0:\n",
      "  (ff_0 output has 16 dimensions)\n",
      "    Neuron  0 impact: +0.000119\n",
      "    Neuron  4 impact: -0.002772\n",
      "    Neuron  8 impact: -0.001054\n",
      "    Neuron 12 impact: -0.000860\n",
      "\n",
      "  Testing replacement strategies on neuron 5:\n",
      "    zero   replacement impact: -0.001142\n",
      "    mean   replacement impact: -0.001135\n",
      "    random replacement impact: -0.005381\n",
      "\n",
      "3. LOGIT LENS TEST\n",
      "----------------------------------------\n",
      "  Testing logit lens at different layers:\n",
      "    embed: predicts token 1 at last position\n",
      "    attn_0: predicts token 6 at last position\n",
      "    ff_0: predicts token 6 at last position\n",
      "\n",
      "4. ATTENTION PATTERN DETECTION TEST\n",
      "----------------------------------------\n",
      "  Layer 0 attention patterns detected:\n",
      "    Average diagonal (self): 0.112\n",
      "    Average previous token:  0.101\n",
      "    Average first token:     0.108\n",
      "    Average uniformity:      0.925\n",
      "\n",
      "============================================================\n",
      "Mechanistic Interpretability Toolkit Complete!\n",
      "All core functions are working correctly.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TEST YOUR COMPLETE MECHANISTIC INTERPRETABILITY TOOLKIT\n",
    "def test_complete_toolkit():\n",
    "    \"\"\"Test all the tools you've implemented!\"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"Testing Complete Mechanistic Interpretability Toolkit\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    set_seed(42)\n",
    "    model = TinyTransformer()\n",
    "    \n",
    "    # Simple metric function\n",
    "    def confidence_metric(logits, input_ids):\n",
    "        \"\"\"Higher confidence = lower entropy.\"\"\"\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum(dim=-1).mean()\n",
    "        return -entropy.item()  # Negative so higher is better\n",
    "    \n",
    "    print(\"\\n1. ACTIVATION PATCHING TEST\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test inputs\n",
    "    clean_input = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "    corrupted_input = torch.randint(0, 10, (1, 8))\n",
    "    \n",
    "    # Collect activations\n",
    "    cache_clean = ActivationCache().register_hooks(model)\n",
    "    with torch.no_grad(): _ = model(clean_input)\n",
    "    cache_clean.remove_hooks()\n",
    "    \n",
    "    cache_corrupted = ActivationCache().register_hooks(model)\n",
    "    with torch.no_grad(): _ = model(corrupted_input)\n",
    "    cache_corrupted.remove_hooks()\n",
    "    \n",
    "    # Test patching\n",
    "    patched, baseline, clean = activation_patching(\n",
    "        model, clean_input, corrupted_input, 'attn_0',\n",
    "        cache_clean, cache_corrupted, confidence_metric\n",
    "    )\n",
    "    \n",
    "    print(f\"  Baseline (corrupted): {baseline:.4f}\")\n",
    "    print(f\"  After patching attn_0: {patched:.4f}\")\n",
    "    print(f\"  Clean target:         {clean:.4f}\")\n",
    "    print(f\"  Patching {'works!' if patched != baseline else 'needs debugging'}\")\n",
    "    \n",
    "    print(\"\\n2. ABLATION STUDY TEST\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    test_input = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "    \n",
    "    # Compute mean cache for proper mean ablation\n",
    "    print(\"  Computing mean activation cache...\")\n",
    "    mean_cache = compute_mean_cache(model)\n",
    "    \n",
    "    # Test individual neuron importance in ff_0\n",
    "    print(\"  Testing neuron importance in ff_0:\")\n",
    "    print(f\"  (ff_0 output has {model.d_model} dimensions)\")\n",
    "    neurons_to_test = [0, 4, 8, 12]  # Valid indices for 16-dim output\n",
    "    \n",
    "    for neuron in neurons_to_test:\n",
    "        impact = measure_ablation_impact(\n",
    "            model, test_input, 'ff_0', neuron, confidence_metric, mean_cache\n",
    "        )\n",
    "        print(f\"    Neuron {neuron:2d} impact: {impact:+.6f}\")\n",
    "    \n",
    "    # Test different replacement strategies\n",
    "    print(\"\\n  Testing replacement strategies on neuron 5:\")\n",
    "    \n",
    "    # Baseline\n",
    "    with torch.no_grad():\n",
    "        normal_out = model(test_input)[0]\n",
    "        normal_conf = confidence_metric(normal_out, test_input)\n",
    "    \n",
    "    for strategy in ['zero', 'mean', 'random']:\n",
    "        # Create appropriate hook\n",
    "        if strategy == 'mean':\n",
    "            hook_handle = ablate_neurons(model, 'ff_0', [5], strategy, mean_cache)\n",
    "        else:\n",
    "            hook_handle = ablate_neurons(model, 'ff_0', [5], strategy)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ablated_out = model(test_input)[0]\n",
    "            ablated_conf = confidence_metric(ablated_out, test_input)\n",
    "        \n",
    "        hook_handle.remove()\n",
    "        \n",
    "        impact = normal_conf - ablated_conf\n",
    "        print(f\"    {strategy:6s} replacement impact: {impact:+.6f}\")\n",
    "    \n",
    "    print(\"\\n3. LOGIT LENS TEST\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test logit lens with visualization\n",
    "    cache = ActivationCache()\n",
    "    cache.register_hooks(model)\n",
    "    \n",
    "    test_seq = torch.tensor([[1, 2, 3, 4, 1, 2, 3, 4]])\n",
    "    with torch.no_grad():\n",
    "        _ = model(test_seq)\n",
    "    \n",
    "    print(\"  Testing logit lens at different layers:\")\n",
    "    for layer_name in ['embed', 'attn_0', 'ff_0']:\n",
    "        if layer_name in cache.activations:\n",
    "            logits = logit_lens(model, cache.activations[layer_name])\n",
    "            top_pred = logits[0, -1].argmax().item()\n",
    "            print(f\"    {layer_name}: predicts token {top_pred} at last position\")\n",
    "    \n",
    "    cache.remove_hooks()\n",
    "    \n",
    "    print(\"\\n4. ATTENTION PATTERN DETECTION TEST\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    test_input = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]])\n",
    "    logits, attn_maps = model(test_input)\n",
    "    \n",
    "    if len(attn_maps) > 0:\n",
    "        patterns = identify_attention_patterns(attn_maps[0])\n",
    "        print(\"  Layer 0 attention patterns detected:\")\n",
    "        print(f\"    Average diagonal (self): {np.mean(patterns['diagonal']):.3f}\")\n",
    "        print(f\"    Average previous token:  {np.mean(patterns['previous']):.3f}\")\n",
    "        print(f\"    Average first token:     {np.mean(patterns['first']):.3f}\")\n",
    "        print(f\"    Average uniformity:      {np.mean(patterns['uniform']):.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Mechanistic Interpretability Toolkit Complete!\")\n",
    "    print(\"All core functions are working correctly.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Run the complete test\n",
    "test_complete_toolkit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "nvz0iitdg3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì PyTorch Hooks Tutorial\n",
      "==================================================\n",
      "1. BASIC HOOK STRUCTURE:\n",
      "------------------------------\n",
      "\n",
      "    def my_hook(module, input, output):\n",
      "        # module: the layer being hooked\n",
      "        # input: tuple of inputs to this layer\n",
      "        # output: what the layer computed\n",
      "        \n",
      "        # You can modify output here\n",
      "        modified_output = output * 2  # Example: double everything\n",
      "        \n",
      "        # MUST return the (possibly modified) output\n",
      "        return modified_output\n",
      "    \n",
      "\n",
      "2. HOOK LIFECYCLE:\n",
      "------------------------------\n",
      "Running forward pass with shape printer hooks:\n",
      "  Layer Linear: torch.Size([2, 8])\n",
      "  Layer ReLU: torch.Size([2, 8])\n",
      "  Layer Linear: torch.Size([2, 4])\n",
      "\n",
      "3. MODIFYING SPECIFIC NEURONS:\n",
      "------------------------------\n",
      "Before ablation:\n",
      "  Output sum: 2.21\n",
      "\n",
      "After ablating neurons [2, 3] in layer 0:\n",
      "  Output sum: 2.21\n",
      "  Difference: 0.00\n",
      "\n",
      "üí° Key Points:\n",
      "  ‚Ä¢ Hooks must RETURN the (modified) output\n",
      "  ‚Ä¢ Use clone() to avoid in-place modifications\n",
      "  ‚Ä¢ The function returns a handle - save it to remove later\n",
      "  ‚Ä¢ Hooks persist until removed with handle.remove()\n"
     ]
    }
   ],
   "source": [
    "# UNDERSTANDING PYTORCH HOOKS - Simple Example\n",
    "def hook_tutorial():\n",
    "    \"\"\"Let's understand how hooks work with a simple example.\"\"\"\n",
    "    print(\"üéì PyTorch Hooks Tutorial\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a simple model\n",
    "    simple_model = nn.Sequential(\n",
    "        nn.Linear(4, 8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8, 4)\n",
    "    )\n",
    "    \n",
    "    print(\"1. BASIC HOOK STRUCTURE:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"\"\"\n",
    "    def my_hook(module, input, output):\n",
    "        # module: the layer being hooked\n",
    "        # input: tuple of inputs to this layer\n",
    "        # output: what the layer computed\n",
    "        \n",
    "        # You can modify output here\n",
    "        modified_output = output * 2  # Example: double everything\n",
    "        \n",
    "        # MUST return the (possibly modified) output\n",
    "        return modified_output\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\n2. HOOK LIFECYCLE:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Example: Print shapes as data flows through\n",
    "    def shape_printer_hook(module, input, output):\n",
    "        print(f\"  Layer {module.__class__.__name__}: {output.shape}\")\n",
    "        return output  # Return unchanged\n",
    "    \n",
    "    # Register hooks\n",
    "    handles = []\n",
    "    for layer in simple_model:\n",
    "        handle = layer.register_forward_hook(shape_printer_hook)\n",
    "        handles.append(handle)\n",
    "    \n",
    "    # Run forward pass\n",
    "    print(\"Running forward pass with shape printer hooks:\")\n",
    "    x = torch.randn(2, 4)\n",
    "    _ = simple_model(x)\n",
    "    \n",
    "    # Clean up hooks\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    print(\"\\n3. MODIFYING SPECIFIC NEURONS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    def selective_ablation_hook(neuron_indices):\n",
    "        \"\"\"Returns a hook that zeros specific neurons.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # Clone to avoid modifying original\n",
    "            modified = output.clone()\n",
    "            \n",
    "            # Zero out specific neurons (last dimension)\n",
    "            modified[..., neuron_indices] = 0\n",
    "            \n",
    "            return modified\n",
    "        return hook\n",
    "    \n",
    "    # Example: Zero out neurons 2 and 3 in first layer\n",
    "    handle = simple_model[0].register_forward_hook(\n",
    "        selective_ablation_hook([2, 3])\n",
    "    )\n",
    "    \n",
    "    print(\"Before ablation:\")\n",
    "    output_normal = simple_model(x)\n",
    "    print(f\"  Output sum: {output_normal.sum():.2f}\")\n",
    "    \n",
    "    handle.remove()  # Remove old hook\n",
    "    \n",
    "    # Now with ablation\n",
    "    handle = simple_model[0].register_forward_hook(\n",
    "        selective_ablation_hook([2, 3])\n",
    "    )\n",
    "    \n",
    "    print(\"\\nAfter ablating neurons [2, 3] in layer 0:\")\n",
    "    output_ablated = simple_model(x)\n",
    "    print(f\"  Output sum: {output_ablated.sum():.2f}\")\n",
    "    print(f\"  Difference: {(output_normal - output_ablated).abs().sum():.2f}\")\n",
    "    \n",
    "    handle.remove()\n",
    "    \n",
    "    print(\"\\nüí° Key Points:\")\n",
    "    print(\"  ‚Ä¢ Hooks must RETURN the (modified) output\")\n",
    "    print(\"  ‚Ä¢ Use clone() to avoid in-place modifications\")\n",
    "    print(\"  ‚Ä¢ The function returns a handle - save it to remove later\")\n",
    "    print(\"  ‚Ä¢ Hooks persist until removed with handle.remove()\")\n",
    "\n",
    "# Run the tutorial\n",
    "hook_tutorial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4036766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
