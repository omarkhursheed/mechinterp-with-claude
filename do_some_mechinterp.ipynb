{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a9fb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds set. Ready for deterministic exploration.\n"
     ]
    }
   ],
   "source": [
    "# experiment.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Set ALL the seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    # Your turn: what seeds do we need to set?\n",
    "    # Think about what could introduce randomness\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    print(\"Seeds set. Ready for deterministic exploration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f161998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3367, 0.1288, 0.2345])\n",
      "tensor([0.3367, 0.1288, 0.2345])\n",
      "Identical? True\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_seed(42)\n",
    "    a = torch.randn(3)\n",
    "    print(a)\n",
    "    set_seed(42)\n",
    "    b = torch.randn(3)\n",
    "    print(b)\n",
    "    print(f\"Identical? {torch.equal(a, b)}\")  # Should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7b30cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=16, n_heads=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads  # 16/2 = 8 dims per head\n",
    "        \n",
    "        # Create Q, K, V projections\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_Q(x)  # [batch, seq, d_model]\n",
    "        K = self.W_K(x)  # You implement\n",
    "        V = self.W_V(x) # You implement\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # We need [batch, n_heads, seq, d_head]\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2) # You reshape K the same way\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)  # You reshape V the same way\n",
    "        \n",
    "            # Compute attention scores\n",
    "        scores = Q @ K.transpose(2,3) / (self.d_head ** 0.5)        \n",
    "        # Apply attention to values\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # How do you turn scores into probabilities?\n",
    "        self.last_attn_weights = attn_weights  # Save for analysis!\n",
    "        attn_output = attn_weights @ V  # How do you apply weights to V?\n",
    "        attn_output = attn_output.transpose(1,2).reshape(batch_size, seq_len, d_model)\n",
    "\n",
    "        attn_output = self.W_O(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983372f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model=16, n_heads=2, d_ff=64):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feedforward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Attention block with residual\n",
    "        attn_out, attn_weights = self.attn(x)\n",
    "        x = x + attn_out  # How do you combine x and attn_out (residual)?\n",
    "        x = self.norm1(x)  # Apply layer norm\n",
    "        \n",
    "        # Feedforward block with residual  \n",
    "        ff_out = self.ff(x)\n",
    "        x = x + ff_out  # Residual connection\n",
    "        x = self.norm2(x)  # Layer norm\n",
    "        \n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5332e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyTransformer(nn.Module):\n",
    "    def __init__(self, d_model=16, n_heads=2, n_layers=2, vocab_size=10, seq_len=8):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.token_embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embed = nn.Embedding(seq_len, d_model)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, n_heads, d_ff=4*d_model) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        # Get embeddings\n",
    "        tok_emb = self.token_embed(x)\n",
    "        positions = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.pos_embed(positions)\n",
    "        \n",
    "        # Combine\n",
    "        embeddings = tok_emb + pos_emb\n",
    "        \n",
    "        # Pass through layers, collecting attention weights\n",
    "        hidden = embeddings\n",
    "        attention_maps = []\n",
    "        for layer in self.layers:\n",
    "            hidden, attn_weights = layer(hidden)\n",
    "            attention_maps.append(attn_weights)\n",
    "            \n",
    "        logits = self.output(hidden)\n",
    "        return logits, attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "502fe1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: torch.Size([2, 8, 10])\n",
      "Attention maps: 2 layers, each torch.Size([2, 2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "model = TinyTransformer()\n",
    "test_input = torch.randint(0, 10, (2, 8))\n",
    "logits, attn_maps = model(test_input)\n",
    "print(f\"Logits: {logits.shape}\")\n",
    "print(f\"Attention maps: {len(attn_maps)} layers, each {attn_maps[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a335227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerLayer(\n",
       "  (attn): MultiHeadAttention(\n",
       "    (W_Q): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (W_K): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (W_V): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (W_O): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "  (ff): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e803c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# MECHANISTIC INTERPRETABILITY TOOLKIT\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Day 1: Core observation and intervention tools\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Tuple\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "def create_probe_sequences():\n",
    "    \"\"\"Create sequences to test what neurons respond to\"\"\"\n",
    "    probes = {}\n",
    "    \n",
    "    # Position probes - same token in different positions\n",
    "    probes['position'] = torch.tensor([[5,5,5,5,5,5,5,5]])  # All same token\n",
    "    \n",
    "    # Token identity probes - different tokens  \n",
    "    probes['token'] = torch.tensor([[0,1,2,3,4,5,6,7]])  # Different tokens\n",
    "    \n",
    "    # Specific position-token combinations\n",
    "    # Token 7 at position 3 only\n",
    "    probes['position_token'] = torch.tensor([[0,0,0,7,0,0,0,0]])\n",
    "    \n",
    "    return probes\n",
    "\n",
    "# ============================================================================\n",
    "# MECHANISTIC INTERPRETABILITY TOOLKIT\n",
    "# Day 1: Core observation and intervention tools\n",
    "# ============================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddeb7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ACTIVATION CACHE - Spy on the forward pass\n",
    "class ActivationCache:\n",
    "    \"\"\"\n",
    "    Non-invasive activation capture using PyTorch hooks.\n",
    "    This lets us observe what happens inside the model during forward passes.\n",
    "    \n",
    "    Key concepts:\n",
    "    - Hooks don't modify the computation, just observe\n",
    "    - We detach tensors to avoid memory issues\n",
    "    - Can capture any layer's input/output\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.activations = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "    def create_hook(self, name):\n",
    "        \"\"\"Create a hook function that stores activations.\"\"\"\n",
    "        def hook(module, input, output):\n",
    "            # TODO(human): Store the activation, but detach it from the computation graph\n",
    "            # Why detach? To save memory and prevent gradient tracking\n",
    "            # Hint: if output is a tuple (like attention returns), handle both cases\n",
    "            # Store in self.activations[name]\n",
    "            if isinstance(output, tuple):\n",
    "                self.activations[name] = output[0].clone().detach()\n",
    "            else:\n",
    "                self.activations[name] = output.clone().detach()\n",
    "\n",
    "        return hook\n",
    "    \n",
    "    def register_hooks(self, model):\n",
    "        \"\"\"Register hooks on specific layers we want to observe.\"\"\"\n",
    "        # Hook into attention layers\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            # Attention outputs\n",
    "            hook = layer.attn.register_forward_hook(self.create_hook(f'attn_{i}'))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "            # TODO(human): Also register a hook for the feedforward layer\n",
    "            # layer.ff is the feedforward network\n",
    "            # Name it f'ff_{i}'\n",
    "            hook = layer.ff.register_forward_hook(self.create_hook(f'ff_{i}'))\n",
    "            self.hooks.append(hook)\n",
    "            \n",
    "        # Hook into embeddings\n",
    "        hook = model.token_embed.register_forward_hook(self.create_hook('embed'))\n",
    "        self.hooks.append(hook)\n",
    "        \n",
    "        # Hook into final output\n",
    "        hook = model.output.register_forward_hook(self.create_hook('logits'))\n",
    "        self.hooks.append(hook)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self.activations = {}\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks from the model.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"Easy access to activations.\"\"\"\n",
    "        return self.activations[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3oiw09oyw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LOGIT LENS - Decode hidden states into vocabulary\n",
    "def logit_lens(model, hidden_states, layer_idx=None):\n",
    "    \"\"\"\n",
    "    The logit lens lets us interpret any hidden state as a distribution over tokens.\n",
    "    \n",
    "    Key insight: The residual stream at ANY point contains interpretable information!\n",
    "    We can decode it by applying the final transformations (LN + unembedding).\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        hidden_states: Activations from any layer [batch, seq, d_model]\n",
    "        layer_idx: Which layer these states are from (for proper normalization)\n",
    "    \n",
    "    Returns:\n",
    "        logits: Distribution over vocabulary [batch, seq, vocab_size]\n",
    "    \"\"\"\n",
    "    # TODO(human): Apply the final layer norm and output projection\n",
    "    # Steps:\n",
    "    # 1. If the model has a final layer norm, apply it to hidden_states\n",
    "    # 2. Project through model.output to get logits\n",
    "    # Think: Why do we need layer norm before the output layer?\n",
    "    # print(model.output(hidden_states))\n",
    "    return model.output(hidden_states)\n",
    "def visualize_logit_lens(model, input_ids, cache, top_k=5):\n",
    "    \"\"\"\n",
    "    Visualize what tokens the model is 'thinking about' at each layer.\n",
    "    \n",
    "    Shows top-k predicted tokens at each position and layer.\n",
    "    \"\"\"\n",
    "    # TODO(human): For each layer's activations in the cache:\n",
    "    # 1. Apply logit lens to get predictions\n",
    "    # 2. Find top-k tokens at each position\n",
    "    # 3. Create a visualization showing the evolution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5gzwyy30he7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: [1, 2, 3, 4, 1, 2, 3, 4]\n",
      "Input shape: torch.Size([1, 8])\n",
      "\n",
      "ðŸ” Logit Lens Analysis - What is the model thinking?\n",
      "--------------------------------------------------\n",
      "torch.Size([1, 8, 10])\n",
      "After embeddings, shape: torch.Size([1, 8, 10])\n",
      "  Top predicted token at last position: 1\n",
      "\n",
      "After attention layer 0:\n",
      "  Top predicted token at last position: 6\n",
      "\n",
      "After feedforward layer 0:\n",
      "  Top predicted token at last position: 6\n",
      "\n",
      "Final model output:\n",
      "  Top predicted token at last position: 8\n",
      "\n",
      "ðŸ’¡ Notice: Even a random model has preferences at each layer!\n",
      "This is evidence of pre-existing structure in random networks.\n"
     ]
    }
   ],
   "source": [
    "# Test the logit lens - see what the model is \"thinking\"\n",
    "set_seed(42)\n",
    "model = TinyTransformer()\n",
    "cache = ActivationCache()\n",
    "cache.register_hooks(model)\n",
    "\n",
    "# Create a simple test sequence\n",
    "test_input = torch.tensor([[1, 2, 3, 4, 1, 2, 3, 4]])  # Repeating pattern\n",
    "print(f\"Input sequence: {test_input[0].tolist()}\")\n",
    "print(f\"Input shape: {test_input.size()}\")\n",
    "# Run forward pass to collect activations\n",
    "logits, attn_maps = model(test_input)\n",
    "\n",
    "# Now use logit lens at different layers\n",
    "print(\"\\nðŸ” Logit Lens Analysis - What is the model thinking?\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check embeddings\n",
    "embed_logits = logit_lens(model, cache['embed'])\n",
    "print(f\"After embeddings, shape: {embed_logits.shape}\")\n",
    "top_tokens_embed = embed_logits[0, -1].argmax()  # Last position prediction\n",
    "print(f\"  Top predicted token at last position: {top_tokens_embed.item()}\")\n",
    "\n",
    "# Check after first attention layer\n",
    "attn0_logits = logit_lens(model, cache['attn_0'])\n",
    "top_tokens_attn0 = attn0_logits[0, -1].argmax()\n",
    "print(f\"\\nAfter attention layer 0:\")\n",
    "print(f\"  Top predicted token at last position: {top_tokens_attn0.item()}\")\n",
    "\n",
    "# Check after first feedforward\n",
    "ff0_logits = logit_lens(model, cache['ff_0'])\n",
    "top_tokens_ff0 = ff0_logits[0, -1].argmax()\n",
    "print(f\"\\nAfter feedforward layer 0:\")\n",
    "print(f\"  Top predicted token at last position: {top_tokens_ff0.item()}\")\n",
    "\n",
    "# Final output\n",
    "final_top = logits[0, -1].argmax()\n",
    "print(f\"\\nFinal model output:\")\n",
    "print(f\"  Top predicted token at last position: {final_top.item()}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice: Even a random model has preferences at each layer!\")\n",
    "print(\"This is evidence of pre-existing structure in random networks.\")\n",
    "\n",
    "# Clean up\n",
    "cache.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4feaexqhtwi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ATTENTION VISUALIZATION - See what the model is looking at\n",
    "def visualize_attention_patterns(attention_weights, input_ids=None, layer_idx=0, head_idx=None):\n",
    "    \"\"\"\n",
    "    Visualize attention patterns to understand information flow.\n",
    "    \n",
    "    Common patterns to look for:\n",
    "    - Diagonal: attending to self\n",
    "    - Previous token: attending to the token before\n",
    "    - Induction: copying patterns from earlier in sequence\n",
    "    - Global: attending to specific positions (e.g., first token)\n",
    "    \n",
    "    Args:\n",
    "        attention_weights: [batch, n_heads, seq_len, seq_len] or [batch, seq_len, seq_len]\n",
    "        input_ids: Optional token ids for labels\n",
    "        layer_idx: Which layer we're visualizing\n",
    "        head_idx: Specific head to visualize (None = average all heads)\n",
    "    \"\"\"\n",
    "    # Handle different shapes\n",
    "    if len(attention_weights.shape) == 4:\n",
    "        batch_size, n_heads, seq_len, _ = attention_weights.shape\n",
    "        if head_idx is not None:\n",
    "            attn = attention_weights[0, head_idx].detach().cpu().numpy()\n",
    "            title = f\"Layer {layer_idx}, Head {head_idx}\"\n",
    "        else:\n",
    "            # Average over heads\n",
    "            attn = attention_weights[0].mean(dim=0).detach().cpu().numpy()\n",
    "            title = f\"Layer {layer_idx}, Averaged Heads\"\n",
    "    else:\n",
    "        attn = attention_weights[0].detach().cpu().numpy()\n",
    "        title = f\"Layer {layer_idx}\"\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attn, cmap='Blues', cbar=True, square=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Position (attending to)\")\n",
    "    plt.ylabel(\"Position (attending from)\")\n",
    "    \n",
    "    # Add token labels if provided\n",
    "    if input_ids is not None:\n",
    "        tokens = input_ids[0].cpu().numpy()\n",
    "        plt.xticks(range(len(tokens)), tokens, rotation=45)\n",
    "        plt.yticks(range(len(tokens)), tokens)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def identify_attention_patterns(attention_weights, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Automatically identify common attention patterns.\n",
    "    \"\"\"\n",
    "    # Handle both 3D and 4D tensors\n",
    "    if len(attention_weights.shape) == 4:\n",
    "        # [batch, n_heads, seq, seq]\n",
    "        batch, n_heads, seq_len, _ = attention_weights.shape\n",
    "        attn = attention_weights[0]  # Take first batch item\n",
    "    else:\n",
    "        # [batch, seq, seq]\n",
    "        attn = attention_weights[0].unsqueeze(0)  # Add head dimension\n",
    "        n_heads = 1\n",
    "\n",
    "    patterns = {\n",
    "        'diagonal': [],\n",
    "        'previous': [],\n",
    "        'first': [],\n",
    "        'uniform': []\n",
    "    }\n",
    "\n",
    "    for head_idx in range(attn.shape[0]):\n",
    "        head_attn = attn[head_idx]  # [seq, seq]\n",
    "\n",
    "        # Diagonal (self-attention)\n",
    "        diagonal_avg = head_attn.diagonal().mean().item()\n",
    "        patterns['diagonal'].append(diagonal_avg)\n",
    "\n",
    "        # Previous token (offset=-1 diagonal)\n",
    "        if seq_len > 1:\n",
    "            prev_diag = head_attn.diagonal(offset=-1)\n",
    "            previous_avg = prev_diag.mean().item() if len(prev_diag) > 0 else 0\n",
    "        else:\n",
    "            previous_avg = 0\n",
    "        patterns['previous'].append(previous_avg)\n",
    "\n",
    "        # First token attention (column 0)\n",
    "        first_avg = head_attn[:, 0].mean().item()\n",
    "        patterns['first'].append(first_avg)\n",
    "\n",
    "        # Uniformity (lower std = more uniform)\n",
    "        # Perfect uniform would be 1/seq_len everywhere, std would be 0\n",
    "        uniformity = 1 - head_attn.std().item()  # Simple std across all values\n",
    "        patterns['uniform'].append(uniformity)\n",
    "\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ux5e958d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Attention Pattern Analysis\n",
      "==================================================\n",
      "\n",
      "REPEATING sequence: [1, 2, 3, 4, 1, 2, 3, 4]\n",
      "----------------------------------------\n",
      "\n",
      "Layer 0:\n",
      "  Head 0:\n",
      "    Self-attention: 0.138\n",
      "    Previous token: 0.105\n",
      "    First token:    0.141\n",
      "    Uniformity:     0.941\n",
      "    â†’ Dominant: first attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.078\n",
      "    Previous token: 0.117\n",
      "    First token:    0.087\n",
      "    Uniformity:     0.930\n",
      "    â†’ Dominant: previous attention\n",
      "\n",
      "Layer 1:\n",
      "  Head 0:\n",
      "    Self-attention: 0.119\n",
      "    Previous token: 0.143\n",
      "    First token:    0.129\n",
      "    Uniformity:     0.937\n",
      "    â†’ Dominant: previous attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.117\n",
      "    Previous token: 0.117\n",
      "    First token:    0.109\n",
      "    Uniformity:     0.968\n",
      "    â†’ Dominant: self attention\n",
      "\n",
      "ASCENDING sequence: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "----------------------------------------\n",
      "\n",
      "Layer 0:\n",
      "  Head 0:\n",
      "    Self-attention: 0.121\n",
      "    Previous token: 0.121\n",
      "    First token:    0.127\n",
      "    Uniformity:     0.913\n",
      "    â†’ Dominant: first attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.083\n",
      "    Previous token: 0.107\n",
      "    First token:    0.074\n",
      "    Uniformity:     0.906\n",
      "    â†’ Dominant: previous attention\n",
      "\n",
      "Layer 1:\n",
      "  Head 0:\n",
      "    Self-attention: 0.105\n",
      "    Previous token: 0.148\n",
      "    First token:    0.091\n",
      "    Uniformity:     0.937\n",
      "    â†’ Dominant: previous attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.118\n",
      "    Previous token: 0.135\n",
      "    First token:    0.127\n",
      "    Uniformity:     0.958\n",
      "    â†’ Dominant: previous attention\n",
      "\n",
      "CONSTANT sequence: [5, 5, 5, 5, 5, 5, 5, 5]\n",
      "----------------------------------------\n",
      "\n",
      "Layer 0:\n",
      "  Head 0:\n",
      "    Self-attention: 0.127\n",
      "    Previous token: 0.109\n",
      "    First token:    0.121\n",
      "    Uniformity:     0.967\n",
      "    â†’ Dominant: self attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.092\n",
      "    Previous token: 0.130\n",
      "    First token:    0.150\n",
      "    Uniformity:     0.921\n",
      "    â†’ Dominant: first attention\n",
      "\n",
      "Layer 1:\n",
      "  Head 0:\n",
      "    Self-attention: 0.127\n",
      "    Previous token: 0.141\n",
      "    First token:    0.123\n",
      "    Uniformity:     0.969\n",
      "    â†’ Dominant: previous attention\n",
      "  Head 1:\n",
      "    Self-attention: 0.125\n",
      "    Previous token: 0.133\n",
      "    First token:    0.154\n",
      "    Uniformity:     0.970\n",
      "    â†’ Dominant: first attention\n",
      "\n",
      "ðŸ’¡ Even random models show structured attention patterns!\n"
     ]
    }
   ],
   "source": [
    "# Test attention pattern identification\n",
    "set_seed(42)\n",
    "model = TinyTransformer()\n",
    "\n",
    "# Create different test sequences to see different patterns\n",
    "test_sequences = {\n",
    "    'repeating': torch.tensor([[1, 2, 3, 4, 1, 2, 3, 4]]),\n",
    "    'ascending': torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7]]),\n",
    "    'constant': torch.tensor([[5, 5, 5, 5, 5, 5, 5, 5]])\n",
    "}\n",
    "\n",
    "print(\"ðŸ” Attention Pattern Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for seq_name, seq in test_sequences.items():\n",
    "    print(f\"\\n{seq_name.upper()} sequence: {seq[0].tolist()}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get attention patterns\n",
    "    logits, attn_maps = model(seq)\n",
    "    \n",
    "    # Analyze each layer\n",
    "    for layer_idx, attn in enumerate(attn_maps):\n",
    "        patterns = identify_attention_patterns(attn)\n",
    "        \n",
    "        print(f\"\\nLayer {layer_idx}:\")\n",
    "        for head_idx in range(len(patterns['diagonal'])):\n",
    "            print(f\"  Head {head_idx}:\")\n",
    "            print(f\"    Self-attention: {patterns['diagonal'][head_idx]:.3f}\")\n",
    "            print(f\"    Previous token: {patterns['previous'][head_idx]:.3f}\")\n",
    "            print(f\"    First token:    {patterns['first'][head_idx]:.3f}\")\n",
    "            print(f\"    Uniformity:     {patterns['uniform'][head_idx]:.3f}\")\n",
    "            \n",
    "            # Identify dominant pattern\n",
    "            scores = {\n",
    "                'self': patterns['diagonal'][head_idx],\n",
    "                'previous': patterns['previous'][head_idx],\n",
    "                'first': patterns['first'][head_idx]\n",
    "            }\n",
    "            dominant = max(scores, key=scores.get)\n",
    "            print(f\"    â†’ Dominant: {dominant} attention\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Even random models show structured attention patterns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "az4uvhz2dyl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured activations:\n",
      "  embed: torch.Size([1, 8, 16])\n",
      "  attn_0: torch.Size([1, 8, 16])\n",
      "  ff_0: torch.Size([1, 8, 16])\n",
      "  attn_1: torch.Size([1, 8, 16])\n",
      "  ff_1: torch.Size([1, 8, 16])\n",
      "  logits: torch.Size([1, 8, 10])\n"
     ]
    }
   ],
   "source": [
    "# TEST YOUR IMPLEMENTATION - Morning checkpoint\n",
    "def test_hooks_and_cache():\n",
    "    \"\"\"Test that your hook implementation works correctly.\"\"\"\n",
    "    set_seed(42)\n",
    "    model = TinyTransformer()\n",
    "    cache = ActivationCache()\n",
    "    \n",
    "    # Register hooks\n",
    "    cache.register_hooks(model)\n",
    "    \n",
    "    # Run forward pass\n",
    "    test_input = torch.randint(0, 10, (1, 8))\n",
    "    logits, attn_maps = model(test_input)\n",
    "    \n",
    "    # Check that activations were captured\n",
    "    print(\"Captured activations:\")\n",
    "    for key in cache.activations:\n",
    "        if isinstance(cache.activations[key], tuple):\n",
    "            print(f\"  {key}: {cache.activations[key][0].shape}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {cache.activations[key].shape}\")\n",
    "    \n",
    "    # Clean up\n",
    "    cache.remove_hooks()\n",
    "    return cache\n",
    "\n",
    "# Run this after implementing the TODOs above\n",
    "cache = test_hooks_and_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2slpswuy0py",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test your hook implementation!\n",
    "set_seed(42)\n",
    "model = TinyTransformer()\n",
    "cache = ActivationCache()\n",
    "\n",
    "# Register hooks\n",
    "cache.register_hooks(model)\n",
    "\n",
    "# Run forward pass\n",
    "test_input = torch.randint(0, 10, (1, 8))\n",
    "print(f\"Input sequence: {test_input[0].tolist()}\")\n",
    "\n",
    "logits, attn_maps = model(test_input)\n",
    "\n",
    "# Check what we captured\n",
    "print(\"\\nâœ… Successfully captured activations:\")\n",
    "for key in sorted(cache.activations.keys()):\n",
    "    act = cache.activations[key]\n",
    "    if isinstance(act, tuple):\n",
    "        shape = act[0].shape if hasattr(act[0], 'shape') else type(act[0])\n",
    "    else:\n",
    "        shape = act.shape if hasattr(act, 'shape') else type(act)\n",
    "    print(f\"  {key}: {shape}\")\n",
    "\n",
    "# Clean up\n",
    "cache.remove_hooks()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Hooks working perfectly! You can now observe any layer's activations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5q4y119fdl9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 2: INTERVENTION TOOLS - Understanding causality\n",
    "# ============================================================================\n",
    "\n",
    "# 4. ACTIVATION PATCHING - Test causal importance\n",
    "def activation_patching(model, clean_input, corrupted_input, layer_name, cache_clean, cache_corrupted, metric_fn):\n",
    "    \"\"\"\n",
    "    Activation patching reveals which components are causally important.\n",
    "    \n",
    "    Method:\n",
    "    1. Run model on corrupted input (e.g., with critical info removed)\n",
    "    2. Run model on clean input\n",
    "    3. Replace specific activation from corrupted run with clean activation\n",
    "    4. If performance recovers, that activation was crucial!\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        clean_input: Input with correct information\n",
    "        corrupted_input: Input with information removed/corrupted\n",
    "        layer_name: Which activation to patch (e.g., 'attn_0', 'ff_1')\n",
    "        cache_clean: ActivationCache from clean run\n",
    "        cache_corrupted: ActivationCache from corrupted run\n",
    "        metric_fn: Function to measure performance (higher = better)\n",
    "    \n",
    "    Returns:\n",
    "        patched_metric: Performance after patching\n",
    "        baseline_metric: Performance without patching (corrupted)\n",
    "        clean_metric: Performance on clean input (upper bound)\n",
    "    \"\"\"\n",
    "    # TODO(human): Implement the patching intervention\n",
    "    # Steps:\n",
    "    # 1. Get corrupted activation at layer_name\n",
    "    # 2. Get clean activation at layer_name\n",
    "    # 3. Run model on corrupted input, but replace the activation at layer_name with clean\n",
    "    # 4. Measure performance with metric_fn\n",
    "    # Hint: You'll need to use hooks to replace activations during forward pass\n",
    "    pass\n",
    "\n",
    "# 5. ABLATION STUDIES - Remove components to test importance\n",
    "def ablate_neurons(model, layer_name, neuron_indices, replacement='zero'):\n",
    "    \"\"\"\n",
    "    Ablate (remove) specific neurons to test their importance.\n",
    "    \n",
    "    Replacement strategies:\n",
    "    - 'zero': Set activations to 0\n",
    "    - 'mean': Set to dataset mean activation\n",
    "    - 'random': Replace with random noise\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model\n",
    "        layer_name: Which layer to ablate (e.g., 'ff_0')\n",
    "        neuron_indices: Which neurons to ablate\n",
    "        replacement: How to replace ablated neurons\n",
    "    \"\"\"\n",
    "    # TODO(human): Create a hook that replaces specific neuron activations\n",
    "    # The hook should:\n",
    "    # 1. Check if this is the target layer\n",
    "    # 2. Replace specified neurons with the replacement value\n",
    "    # 3. Return the modified activation\n",
    "    pass\n",
    "\n",
    "def measure_ablation_impact(model, input_ids, layer_name, neuron_idx, metric_fn):\n",
    "    \"\"\"\n",
    "    Measure how much a specific neuron matters.\n",
    "    \n",
    "    Returns:\n",
    "        impact: Change in metric when neuron is ablated\n",
    "    \"\"\"\n",
    "    # Baseline performance\n",
    "    with torch.no_grad():\n",
    "        baseline_output = model(input_ids)[0]\n",
    "        baseline_metric = metric_fn(baseline_output, input_ids)\n",
    "    \n",
    "    # TODO(human): Performance with neuron ablated\n",
    "    # 1. Apply ablation to the neuron\n",
    "    # 2. Run forward pass\n",
    "    # 3. Measure performance\n",
    "    # 4. Calculate impact as (baseline - ablated)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jetxhkf0mwd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: How to use these tools together\n",
    "def example_analysis_pipeline():\n",
    "    \"\"\"\n",
    "    Example showing how to combine all the tools for analysis.\n",
    "    This is what you'll do in your research!\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MECHANISTIC INTERPRETABILITY ANALYSIS PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Setup model and inputs\n",
    "    set_seed(42)\n",
    "    model = TinyTransformer()\n",
    "    \n",
    "    # Create interesting test sequences\n",
    "    # Induction: pattern that repeats\n",
    "    induction_seq = torch.tensor([[1, 2, 3, 4, 1, 2, 3, 4]])  \n",
    "    # Random sequence for comparison\n",
    "    random_seq = torch.randint(0, 10, (1, 8))\n",
    "    \n",
    "    print(\"\\n1. OBSERVING WITH HOOKS\")\n",
    "    print(\"-\" * 40)\n",
    "    # Set up activation cache\n",
    "    cache = ActivationCache()\n",
    "    cache.register_hooks(model)\n",
    "    \n",
    "    # Run forward pass\n",
    "    logits, attn_maps = model(induction_seq)\n",
    "    \n",
    "    print(\"Captured activations from forward pass:\")\n",
    "    for key in sorted(cache.activations.keys()):\n",
    "        act = cache.activations[key]\n",
    "        if isinstance(act, tuple):\n",
    "            print(f\"  {key}: {act[0].shape if hasattr(act[0], 'shape') else type(act[0])}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {act.shape if hasattr(act, 'shape') else type(act)}\")\n",
    "    \n",
    "    print(\"\\n2. ATTENTION PATTERN ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    # Visualize attention for first layer\n",
    "    if len(attn_maps) > 0:\n",
    "        print(\"Visualizing Layer 0 attention patterns...\")\n",
    "        # This would call: visualize_attention_patterns(attn_maps[0], induction_seq, layer_idx=0)\n",
    "        print(\"  (Visualization would appear here)\")\n",
    "    \n",
    "    print(\"\\n3. LOGIT LENS ANALYSIS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"What is the model 'thinking' at each layer?\")\n",
    "    # This would apply logit lens to intermediate activations\n",
    "    # For each layer: logit_lens(model, cache[f'ff_{i}'])\n",
    "    \n",
    "    print(\"\\n4. INTERVENTION EXPERIMENTS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing causal importance via patching...\")\n",
    "    # Example: corrupt the sequence and patch to test importance\n",
    "    corrupted = torch.randint(0, 10, (1, 8))  # Random corruption\n",
    "    # Would run: activation_patching(model, induction_seq, corrupted, 'attn_0', ...)\n",
    "    \n",
    "    print(\"\\n5. ABLATION STUDIES\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Testing neuron importance via ablation...\")\n",
    "    # Would run: measure_ablation_impact(model, induction_seq, 'ff_0', neuron_idx=0, ...)\n",
    "    \n",
    "    # Clean up\n",
    "    cache.remove_hooks()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Analysis complete! Ready for your research experiments.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Uncomment to run the example (after implementing TODOs):\n",
    "# example_analysis_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_stack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
